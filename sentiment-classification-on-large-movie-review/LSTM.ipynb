{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tree import Tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 12500 11000\n"
     ]
    }
   ],
   "source": [
    "pos_dir = 'train/pos'\n",
    "neg_dir = 'train/neg'\n",
    "test_dir = 'test'\n",
    "pos_fs = os.listdir(pos_dir)\n",
    "pos_fs = list(filter(lambda x: '.txt' in x, pos_fs))\n",
    "neg_fs = os.listdir(neg_dir)\n",
    "neg_fs = list(filter(lambda x: '.txt' in x, neg_fs))\n",
    "test_fs = os.listdir(test_dir)\n",
    "test_fs = list(filter(lambda x: '.txt' in x, test_fs))\n",
    "\n",
    "pos_n = len(pos_fs)\n",
    "neg_n = len(neg_fs)\n",
    "test_n = len(test_fs)\n",
    "print(pos_n, neg_n, test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX, trainY = None, None\n",
    "\n",
    "# Fetch rates\n",
    "rates_pos = np.array(list(map(lambda x: int(re.search('_([0-9]+).txt', x).group(1)), pos_fs)))\n",
    "rates_neg = np.array(list(map(lambda x: int(re.search('_([0-9]+).txt', x).group(1)), neg_fs)))\n",
    "\n",
    "word_set = set()\n",
    "word_freq = Counter()\n",
    "word_int = dict()\n",
    "\n",
    "train_token_list = []\n",
    "test_token_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown_threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Handle Special words case\n",
    "special = {\n",
    "    'unknown': '<unk>',\n",
    "    'number': '<nmb>',\n",
    "    'punctuation': '<pun>',\n",
    "    'or': '<or>',\n",
    "    'rate': '<rat>',\n",
    "}\n",
    "\n",
    "puncs = {\n",
    "    '.': '<dot>',\n",
    "    ',': '<com>',\n",
    "    '!': '<exc>',\n",
    "    '?': '<que>',\n",
    "    '\"': '<quo>',\n",
    "    '``': '<quo>',\n",
    "    '\\'\\'': '<quo>'\n",
    "}\n",
    "\n",
    "# Use ne_chunk to handle NER\n",
    "ner_symbols = {\n",
    "    'FACILITY': '<fac>',\n",
    "    'GPE': '<gpe>',\n",
    "    'GSP': '<gsp>',\n",
    "    'LOCATION': '<loc>',\n",
    "    'ORGANIZATION': '<org>',\n",
    "    'PERSON': '<per>'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word preprocessing\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "def lemmatize(word_lemmatizer, pos_tagged):\n",
    "    words = [x[0] for x in pos_tagged]\n",
    "    poses = [x[1] for x in pos_tagged]\n",
    "    wns = list(map(lambda x: penn_to_wn(x), poses))\n",
    "    lemmatized = list(map(lambda args: (args[0] if args[2] is None \n",
    "                    else word_lemmatizer.lemmatize(args[0], args[2]), args[1]), zip(words, poses, wns)))\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ner(pos_tagged):\n",
    "    chunks = ne_chunk(pos_tagged)\n",
    "    idx = 0\n",
    "    ner_info = []\n",
    "    ner_name = None\n",
    "    for i in chunks:\n",
    "        step = 1\n",
    "        if type(i) == Tree:\n",
    "            info = str(i)[1:-1].split()\n",
    "            cur_ner = info[0]\n",
    "            step = len(info[1:])\n",
    "            # Consolidate consecutive same ner    \n",
    "            if cur_ner == ner_name:\n",
    "                ner_info[-1][2] += step\n",
    "            else:\n",
    "                ner_info.append([cur_ner, idx, idx+step])\n",
    "            ner_name = cur_ner\n",
    "        else:\n",
    "            ner_name = None\n",
    "        idx += step\n",
    "\n",
    "    ner_dict = {l:(r, ner) for ner, l, r in ner_info}\n",
    "    return ner_dict\n",
    "\n",
    "\n",
    "def replace_ner(pos_tagged, ner_dict, ner_symbols):\n",
    "    modified = []\n",
    "    idx = 0\n",
    "    while idx < len(pos_tagged):\n",
    "        t, p = pos_tagged[idx]\n",
    "        if idx in ner_dict:\n",
    "            r, ner = ner_dict[idx]\n",
    "            modified.append((ner_symbols[ner], p))\n",
    "            idx = r\n",
    "        else:\n",
    "            modified.append((t, p))\n",
    "            idx += 1\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def is_punc(s):\n",
    "    is_punc = True\n",
    "    for c in s:\n",
    "        if c not in punctuation:\n",
    "            return False\n",
    "    return True \n",
    "\n",
    "\n",
    "def sent_to_tokens(sent, tokens=None):\n",
    "    if not tokens:\n",
    "        tokens = word_tokenize(sent.strip().lower())\n",
    "    res = []\n",
    "    for t in tokens:\n",
    "        if is_number(t):\n",
    "            res.append(special['number'])\n",
    "        elif t in puncs:\n",
    "            res.append(puncs[t])\n",
    "        elif is_punc(t):\n",
    "            res.append(special['punctuation'])\n",
    "        elif re.match(r'[0-9]/[0-9]', t) is not None:\n",
    "            res.append(special['rate'])\n",
    "        elif re.match(r'[0-9]+/[0-9]+', t) is not None:\n",
    "            res.append('<nmb>/<nmb>')\n",
    "        elif t.find('/') != len(t)-1 and t.find('/') != 0:\n",
    "            for st in t.split('/'):\n",
    "                res.append(st)\n",
    "                res.append(special['or'])\n",
    "            del res[-1]\n",
    "        else:\n",
    "            res.append(t)\n",
    "    res2 = []\n",
    "    for t in res:\n",
    "        if t.find('.') !=len(t)-1 and t.find('.') != 0:      \n",
    "            for st in t.split('.'):\n",
    "                res2.append(st)\n",
    "                res2.append(special['or'])\n",
    "            del res2[-1]\n",
    "        else:\n",
    "            res2.append(t)\n",
    "    \n",
    "    res3 = []\n",
    "    for t in res2:\n",
    "        if t.find('-') !=len(t)-1 and t.find('-') != 0:\n",
    "            s = ''\n",
    "            for st in t.split('-'):\n",
    "                if is_number(st):\n",
    "                    s += special['number']\n",
    "                else:\n",
    "                    s += st\n",
    "                s+='-'\n",
    "            s = s[:-1]\n",
    "            res3.append(s)\n",
    "        else:\n",
    "            res3.append(t)\n",
    "    \n",
    "    res4 = []\n",
    "    for t in res3:\n",
    "        if t.find('~') !=len(t)-1 and t.find('~') != 0:\n",
    "            s = ''\n",
    "            for st in t.split('~'):\n",
    "                if is_number(st):\n",
    "                    s += special['number']\n",
    "                else:\n",
    "                    s += st\n",
    "                s+='~'\n",
    "            s = s[:-1]\n",
    "            res4.append(s)\n",
    "        else:\n",
    "            res4.append(t) \n",
    "    return res4\n",
    "\n",
    "\n",
    "def process_sent(sent):\n",
    "    # Preprocess sentence\n",
    "    \n",
    "    # Remove html tags \n",
    "    text = re.sub(r'<[a-zA-Z\\s]*/>', ' ', sent)\n",
    "    # Replace single quotes with double quotes\n",
    "    text = re.sub(r'\\'([a-zA-Z0-9][a-zA-Z0-9\\s]*[a-zA-Z0-9])\\'', r'\"\\1\"', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # POS Tagging\n",
    "    pos_tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Extract NER \n",
    "    ner_dict = get_ner(pos_tagged)\n",
    "    \n",
    "    # Replace NER with unified symbols\n",
    "    modified_pos_tagged = replace_ner(pos_tagged, ner_dict, ner_symbols)\n",
    "    \n",
    "    # Lemmatize (verb present tense, plural->singular etc.)\n",
    "    lemmatized = lemmatize(word_lemmatizer, modified_pos_tagged)\n",
    "    lemmed_tokens = [x[0].lower() for x in lemmatized] \n",
    "    # Number, punctuation, etc. special case handling\n",
    "    \n",
    "    res = sent_to_tokens(sent, tokens=lemmed_tokens)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<per>',\n",
       " '<org>',\n",
       " 'be',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " '<dot>',\n",
       " 'it',\n",
       " 'run',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'program',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " '<com>',\n",
       " 'such',\n",
       " 'as',\n",
       " '<quo>',\n",
       " 'teachers',\n",
       " '<quo>',\n",
       " '<dot>',\n",
       " 'my',\n",
       " '<nmb>',\n",
       " 'year',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'that',\n",
       " '<per>',\n",
       " \"'s\",\n",
       " 'satire',\n",
       " 'be',\n",
       " 'much',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'reality',\n",
       " 'than',\n",
       " 'be',\n",
       " '<quo>',\n",
       " 'teachers',\n",
       " '<quo>',\n",
       " '<dot>',\n",
       " 'the',\n",
       " 'scramble',\n",
       " 'to',\n",
       " 'survive',\n",
       " 'financially',\n",
       " '<com>',\n",
       " 'the',\n",
       " 'insightful',\n",
       " 'student',\n",
       " 'who',\n",
       " 'can',\n",
       " 'see',\n",
       " 'right',\n",
       " 'through',\n",
       " 'their',\n",
       " 'pathetic',\n",
       " 'teacher',\n",
       " '<pun>',\n",
       " 'pomp',\n",
       " '<com>',\n",
       " 'the',\n",
       " 'pettiness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'situation',\n",
       " '<com>',\n",
       " 'all',\n",
       " 'remind',\n",
       " 'me',\n",
       " 'of',\n",
       " 'the',\n",
       " 'school',\n",
       " 'i',\n",
       " 'know',\n",
       " 'and',\n",
       " 'their',\n",
       " 'student',\n",
       " '<dot>',\n",
       " 'when',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'episode',\n",
       " 'in',\n",
       " 'which',\n",
       " 'a',\n",
       " 'student',\n",
       " 'repeatedly',\n",
       " 'try',\n",
       " 'to',\n",
       " 'burn',\n",
       " 'down',\n",
       " 'the',\n",
       " 'school',\n",
       " '<com>',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'recall',\n",
       " '<pun>',\n",
       " '<pun>',\n",
       " '<pun>',\n",
       " 'at',\n",
       " '<pun>',\n",
       " '<pun>',\n",
       " '<pun>',\n",
       " '<dot>',\n",
       " 'high',\n",
       " '<dot>',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'line',\n",
       " '<pun>',\n",
       " 'inspector',\n",
       " '<pun>',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'sack',\n",
       " 'one',\n",
       " 'of',\n",
       " 'your',\n",
       " 'teacher',\n",
       " '<dot>',\n",
       " 'student',\n",
       " '<pun>',\n",
       " 'welcome',\n",
       " 'to',\n",
       " '<org>',\n",
       " '<dot>',\n",
       " 'i',\n",
       " 'expect',\n",
       " 'that',\n",
       " 'many',\n",
       " 'adult',\n",
       " 'of',\n",
       " 'my',\n",
       " 'age',\n",
       " 'think',\n",
       " 'that',\n",
       " '<per>',\n",
       " 'be',\n",
       " 'far',\n",
       " 'fetch',\n",
       " '<dot>',\n",
       " 'what',\n",
       " 'a',\n",
       " 'pity',\n",
       " 'that',\n",
       " 'it',\n",
       " 'be',\n",
       " \"n't\",\n",
       " '<exc>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"\"\"Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\"\"\"\n",
    "# Remove html tags \n",
    "# sent = re.sub(r'<[a-zA-Z\\s]*/>', ' ', sent)\n",
    "#     # Replace single quotes with double quotes\n",
    "# sent = re.sub(r'\\'([a-zA-Z0-9][a-zA-Z0-9\\s]*[a-zA-Z0-9])\\'', r'\"\\1\"', sent)\n",
    "    \n",
    "# tokened = word_tokenize(sent)\n",
    "# pos_tagged = pos_tag(tokened)\n",
    "# pos_tagged\n",
    "process_sent(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_files(dir, files, token_list, is_train=False):\n",
    "    paths = list(map(lambda x: os.path.join(dir, x), files))\n",
    "    for p in paths:\n",
    "        with open(p, 'r') as f:\n",
    "            review = f.read()\n",
    "            tokens = process_sent(review)\n",
    "            token_list.append(tokens)\n",
    "            if is_train:\n",
    "                word_freq.update(Counter(tokens))\n",
    "            \n",
    "            \n",
    "process_files(pos_dir, pos_fs, train_token_list, True)\n",
    "process_files(neg_dir, neg_fs, train_token_list, True)\n",
    "process_files(test_dir, test_fs, test_token_list, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68832"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_int(word_freq):\n",
    "    word_int = dict()\n",
    "    idx = 1\n",
    "    for w in word_freq:\n",
    "        if word_freq[w] <= unknown_threshold:\n",
    "            word_int[w] = 0\n",
    "        else:\n",
    "            word_int[w] = idx\n",
    "            idx += 1\n",
    "    for w in word_int:\n",
    "        if word_int[w] == 0:\n",
    "            word_int[w] = idx\n",
    "    return word_int\n",
    "\n",
    "word_int = word_to_int(word_freq)\n",
    "len(word_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['story', 'of', 'a', 'man', 'who', 'have', 'unnatural', 'feeling', 'for', 'a', 'pig', '<dot>', 'starts', 'out', 'with', 'a', 'opening', 'scene', 'that', 'be', 'a', 'terrific', 'example', 'of', 'absurd', 'comedy', '<dot>', 'a', 'formal', 'orchestra', 'audience', 'be', 'turn', 'into', 'an', 'insane', '<com>', 'violent', 'mob', 'by', 'the', 'crazy', 'chanting', 'of', 'it', \"'s\", 'singer', '<dot>', 'unfortunately', 'it', 'stay', 'absurd', 'the', '<org>', 'time', 'with', 'no', 'general', 'narrative', 'eventually', 'make', 'it', 'just', 'too', 'off', 'put', '<dot>', 'even', 'those', 'from', 'the', 'era', 'should', 'be', 'turn', 'off', '<dot>', 'the', 'cryptic', 'dialogue', 'would', 'make', '<per>', 'seem', 'easy', 'to', 'a', 'third', 'grader', '<dot>', 'on', 'a', 'technical', 'level', 'it', \"'s\", 'good', 'than', 'you', 'might', 'think', 'with', 'some', 'good', 'cinematography', 'by', 'future', 'great', '<per>', '<dot>', '<per>', 'star', '<per>', 'and', '<per>', 'can', 'be', 'see', 'briefly', '<dot>']\n"
     ]
    }
   ],
   "source": [
    "print(train_token_list[12500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index (Vocabulary size) is 21582\n"
     ]
    }
   ],
   "source": [
    "max_idx = 0\n",
    "for k, v in word_int.items():\n",
    "    if v > max_idx:\n",
    "        max_idx = v\n",
    "print('Max index (Vocabulary size) is {0}'.format(max_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_to_sequence(tokens, word_int, max_idx):\n",
    "    res = []\n",
    "    for t in tokens:\n",
    "        if t not in word_int:\n",
    "            res.append(max_idx)\n",
    "        else:\n",
    "            res.append(word_int[t])\n",
    "    return res\n",
    "\n",
    "\n",
    "def batch_sequence(token_list):\n",
    "    x = []\n",
    "    for tokens in token_list:\n",
    "        x.append(token_to_sequence(tokens, word_int, max_freq))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = batch_sequence(train_token_list)\n",
    "\n",
    "testX = batch_sequence(test_token_list)\n",
    "\n",
    "trainY = np.append(np.ones(pos_n, dtype=np.int), np.zeros(neg_n, dtype=np.int))\n",
    "\n",
    "assert len(trainX) == pos_n+neg_n\n",
    "assert len(testX) == test_n\n",
    "assert len(trainY) == pos_n+neg_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Padding post zeros for features\n",
    "# One-hot encoding for Label\n",
    "trainX = pad_sequences(trainX)\n",
    "n, d = trainX.shape\n",
    "testX = pad_sequences(testX, maxlen=d)\n",
    "\n",
    "nb_classes = len(np.unique(trainY))\n",
    "trainY = to_categorical(trainY, nb_classes=nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the train data\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)\n",
    "trainX = trainX[indices]\n",
    "trainY = trainY[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 2649) \n",
      "Validation set: \t(5000, 2649) \n",
      "Test set: \t\t(11000, 2649)\n",
      "label set: \t\t(20000, 2) \n",
      "Validation label set: \t(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "split_frac = 0.8\n",
    "split_index = int(n * split_frac)\n",
    "\n",
    "train_X = trainX[:split_index]\n",
    "val_X = trainX[split_index:]\n",
    "test_X = testX\n",
    "\n",
    "train_Y = trainY[:split_index]\n",
    "val_Y = trainY[split_index:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_X.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_X.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_X.shape))\n",
    "print(\"label set: \\t\\t{}\".format(train_Y.shape), \n",
    "      \"\\nValidation label set: \\t{}\".format(val_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network parameter dimensions setup\n",
    "\n",
    "lstm_size = [256, 512]\n",
    "lstm_layers = 2\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "embedding_dim = 300\n",
    "n, d = train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-b63b3a12a647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xavier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xavier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36mlstm\u001b[0;34m(incoming, n_units, activation, inner_activation, dropout, bias, weights_init, forget_bias, return_seq, return_state, initial_state, dynamic, trainable, restore, reuse, scope, name)\u001b[0m\n\u001b[1;32m    224\u001b[0m                       \u001b[0mreturn_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                       \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                       scope=scope, name=name)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36m_rnn_template\u001b[0;34m(incoming, cell, dropout, return_seq, return_state, initial_state, dynamic, scope, reuse, name)\u001b[0m\n\u001b[1;32m     67\u001b[0m         outputs, state = _rnn(cell, inference, dtype=tf.float32,\n\u001b[1;32m     68\u001b[0m                               \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                               sequence_length=sequence_length)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Retrieve RNN Variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             state_size=cell.state_size)\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0mvarscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    688\u001b[0m                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_keep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                                       seed=self._seed),\n\u001b[0;32m--> 690\u001b[0;31m                 lambda: output)\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 instructions)\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    434\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[0mcontext_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCondContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m     \u001b[0mcontext_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m     \u001b[0morig_res_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildCondBranch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0morig_res_t\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"true_fn must have a return value.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildCondBranch\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1911\u001b[0m     \u001b[0;34m\"\"\"Add the subgraph defined by fn() to the graph.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m     \u001b[0moriginal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_summaries\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_summaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    687\u001b[0m                 lambda: tf.nn.dropout(output,\n\u001b[1;32m    688\u001b[0m                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_keep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                                       seed=self._seed),\n\u001b[0m\u001b[1;32m    690\u001b[0m                 lambda: output)\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(x, keep_prob, noise_shape, seed, name)\u001b[0m\n\u001b[1;32m   2309\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m     \u001b[0mnoise_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_noise_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m     \u001b[0;31m# uniform [keep_prob, 1.0 + keep_prob)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_get_noise_shape\u001b[0;34m(x, noise_shape)\u001b[0m\n\u001b[1;32m   2237\u001b[0m   \u001b[0;31m# If noise_shape is none return immediately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnoise_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2239\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;34m\"\"\"Alias of Tensor.shape.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mneed_shapes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneed_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m           \u001b[0mset_shape_and_handle_data_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape_and_handle_data_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2577\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2581\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2496\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    651\u001b[0m     op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn):\n\u001b[1;32m    652\u001b[0m   \u001b[0;34m\"\"\"Core implementation of call_cpp_shape_fn.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m   \u001b[0mgraph_def_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def_versions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m   \u001b[0mnode_def_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mgraph_def_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3065\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GraphVersions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3067\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3068\u001b[0m       \u001b[0mversion_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversions_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersionDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3069\u001b[0m       \u001b[0mversion_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Network building\n",
    "\n",
    "net = tflearn.input_data([None, d])\n",
    "net = tflearn.embedding(net, input_dim=max_idx, output_dim=embedding_dim)\n",
    "net = tflearn.lstm(net, n_units=lstm_size[0], dropout=0.8, return_seq=True, weights_init='xavier')\n",
    "net = tflearn.lstm(net, n_units=lstm_size[1], dropout=0.8, weights_init='xavier')\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "with tf.device('/gpu:0'):\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "    model.fit(train_X, train_Y, validation_set=(val_X, val_Y), show_metric=True,\n",
    "          batch_size=256, snapshot_epoch=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
