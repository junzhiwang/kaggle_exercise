{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 12500 11000\n"
     ]
    }
   ],
   "source": [
    "pos_dir = 'train/pos'\n",
    "neg_dir = 'train/neg'\n",
    "test_dir = 'test'\n",
    "pos_fs = os.listdir(pos_dir)\n",
    "pos_fs = list(filter(lambda x: '.txt' in x, pos_fs))\n",
    "neg_fs = os.listdir(neg_dir)\n",
    "neg_fs = list(filter(lambda x: '.txt' in x, neg_fs))\n",
    "test_fs = os.listdir(test_dir)\n",
    "test_fs = list(filter(lambda x: '.txt' in x, test_fs))\n",
    "\n",
    "pos_n = len(pos_fs)\n",
    "neg_n = len(neg_fs)\n",
    "test_n = len(test_fs)\n",
    "print(pos_n, neg_n, test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, trainY = None, None\n",
    "\n",
    "# Fetch rates\n",
    "rates_pos = np.array(list(map(lambda x: int(re.search('_([0-9]+).txt', x).group(1)), pos_fs)))\n",
    "rates_neg = np.array(list(map(lambda x: int(re.search('_([0-9]+).txt', x).group(1)), neg_fs)))\n",
    "max_gram = 3\n",
    "\n",
    "# Stats\n",
    "ngram_pos = [[] for i in range(max_gram)]\n",
    "ngram_neg = [[] for i in range(max_gram)]\n",
    "ngram_freq = [{} for i in range(max_gram)]\n",
    "ngram_pos_freq = [{} for i in range(max_gram)]\n",
    "ngram_neg_freq = [{} for i in range(max_gram)]\n",
    "ngram_ratio = [{} for i in range(max_gram)]\n",
    "ngram_idx = [{} for i in range(max_gram)]\n",
    "ngram_weight = [{} for i in range(max_gram)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filters\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('but')\n",
    "puncs_rm = '\\'\"#$!%&()*+,-./:;<=>@[\\\\]^_`{|}~'\n",
    "\n",
    "# Filter by frequency range\n",
    "ngram_low = [10, 9, 8]\n",
    "ngram_high = [12500, 6250, 3125]\n",
    "\n",
    "# Filter by ratio >= #(w in pos)/#(w in neg) or #(w in neg)/#(w in pos)\n",
    "ngram_ratio_threshold = [2, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word preprocessing\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "def lemmatize(word_lemmatizer, tokens):\n",
    "    pos_tagged = pos_tag(tokens)\n",
    "    wns = list(map(lambda x: penn_to_wn(x[1]), pos_tagged))\n",
    "    lemm = list(map(lambda args: word_lemmatizer.lemmatize(args[0]) if args[1] is None \n",
    "                    else word_lemmatizer.lemmatize(args[0], args[1]), zip(tokens, wns)))\n",
    "    return lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 7,  8,  9, 10]), array([2496, 3009, 2263, 4732]))\n",
      "(array([1, 2, 3, 4]), array([5100, 2284, 2420, 2696]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(rates_pos, return_counts=True))\n",
    "print(np.unique(rates_neg, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def is_punc(s):\n",
    "    is_punc = True\n",
    "    for c in s:\n",
    "        if c not in punctuation:\n",
    "            return False\n",
    "    return True \n",
    "\n",
    "\n",
    "def sent_to_tokens(sent, stop_words, puncs_rm, tokens=None):\n",
    "    if not tokens:\n",
    "        tokens = word_tokenize(sent.strip().lower())\n",
    "    res = []\n",
    "    for t in tokens:\n",
    "        if is_number(t) or t.isdigit():\n",
    "            continue\n",
    "        if t in stop_words or is_punc(t):\n",
    "            continue\n",
    "        res.append(t)\n",
    "    return res\n",
    "\n",
    "\n",
    "def sent_to_ngrams(sent, n, stop_words, puncs_rm, tokens=None):\n",
    "    if not tokens:\n",
    "        tokens = word_tokenize(sent.strip().lower())\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        use = False\n",
    "        ngram = tokens[i:i+n]\n",
    "        for g in ngram:\n",
    "            if g not in stop_words and not is_punc(g):\n",
    "                use = True\n",
    "            if is_punc(g) or is_number(g) or g.isdigit():\n",
    "                use = False\n",
    "                break\n",
    "        if use:\n",
    "            ngrams.append(\" \".join(ngram))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def process_sent(sent, max_gram):\n",
    "    # Preprocess sentence\n",
    "    # Remove html tags\n",
    "    sent = re.sub(r'<[a-zA-Z\\s]*/>', ' ', sent)\n",
    "    res = []\n",
    "    tokens = word_tokenize(sent.strip().lower())\n",
    "    tokens = lemmatize(word_lemmatizer, tokens)\n",
    "    res.append(sent_to_tokens(sent, stop_words, puncs_rm, tokens=tokens))\n",
    "    for n in range(2, max_gram+1):\n",
    "        res.append(sent_to_ngrams(sent, n, stop_words, puncs_rm, tokens=tokens))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'be', 'come', '!']"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize('We are coming!')\n",
    "lemmatize(word_lemmatizer, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bromwell',\n",
       "  'high',\n",
       "  'cartoon',\n",
       "  'comedy',\n",
       "  'run',\n",
       "  'time',\n",
       "  'program',\n",
       "  'school',\n",
       "  'life',\n",
       "  'teacher',\n",
       "  'year',\n",
       "  'teaching',\n",
       "  'profession',\n",
       "  'lead',\n",
       "  'believe',\n",
       "  'bromwell',\n",
       "  'high',\n",
       "  \"'s\",\n",
       "  'satire',\n",
       "  'much',\n",
       "  'closer',\n",
       "  'reality',\n",
       "  'teacher',\n",
       "  'scramble',\n",
       "  'survive',\n",
       "  'financially',\n",
       "  'insightful',\n",
       "  'student',\n",
       "  'see',\n",
       "  'right',\n",
       "  'pathetic',\n",
       "  'teacher',\n",
       "  'pomp',\n",
       "  'pettiness',\n",
       "  'whole',\n",
       "  'situation',\n",
       "  'remind',\n",
       "  'school',\n",
       "  'knew',\n",
       "  'student',\n",
       "  'saw',\n",
       "  'episode',\n",
       "  'student',\n",
       "  'repeatedly',\n",
       "  'try',\n",
       "  'burn',\n",
       "  'school',\n",
       "  'immediately',\n",
       "  'recall',\n",
       "  'high',\n",
       "  'classic',\n",
       "  'line',\n",
       "  'inspector',\n",
       "  \"'m\",\n",
       "  'sack',\n",
       "  'one',\n",
       "  'teacher',\n",
       "  'student',\n",
       "  'welcome',\n",
       "  'bromwell',\n",
       "  'high',\n",
       "  'expect',\n",
       "  'many',\n",
       "  'adult',\n",
       "  'age',\n",
       "  'think',\n",
       "  'bromwell',\n",
       "  'high',\n",
       "  'far',\n",
       "  'fetch',\n",
       "  'pity',\n",
       "  \"n't\"]]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_sent(\"\"\"Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, \n",
    "such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much \n",
    "closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right \n",
    "through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and\n",
    "their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately \n",
    "recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. \n",
    "STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. \n",
    "What a pity that it isn't!\"\"\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_freq(grams, n, is_pos):\n",
    "    gram_freq = ngram_freq[n-1]\n",
    "    gram_pos_freq = ngram_pos_freq[n-1]\n",
    "    gram_neg_freq = ngram_neg_freq[n-1]\n",
    "    for g in grams:\n",
    "        if g not in gram_freq:\n",
    "            gram_freq[g] = 1\n",
    "        else:\n",
    "            gram_freq[g] += 1\n",
    "        if is_pos:\n",
    "            if g not in gram_pos_freq:\n",
    "                gram_pos_freq[g] = 1\n",
    "            else:\n",
    "                gram_pos_freq[g] += 1\n",
    "        else:\n",
    "            if g not in gram_neg_freq:\n",
    "                gram_neg_freq[g] = 1\n",
    "            else:\n",
    "                gram_neg_freq[g] += 1\n",
    "    \n",
    "    \n",
    "def process_files(files, dir, is_pos):\n",
    "    for file in files:\n",
    "        with open(os.path.join(dir, file), 'r') as f:\n",
    "            review = f.readline()\n",
    "            res = process_sent(review, max_gram)\n",
    "            for i in range(max_gram):\n",
    "                if is_pos:\n",
    "                    ngram_pos[i].append(res[i])\n",
    "                    add_freq(res[i], i+1, is_pos)\n",
    "                else:\n",
    "                    ngram_neg[i].append(res[i])\n",
    "                    add_freq(res[i], i+1, is_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_files(pos_fs, pos_dir, True)\n",
    "process_files(neg_fs, neg_dir, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"it 's\", 8503), ('the film', 7362), (\"do n't\", 6889), ('this movie', 6680), ('this film', 5364), ('the movie', 5361), ('one of', 4789), ('film be', 3199), ('movie be', 3161), ('the story', 3125)]\n",
      "[(\"do n't\", 10364), ('this movie', 8904), (\"it 's\", 8509), ('the movie', 6769), ('the film', 6334), ('this film', 5462), ('movie be', 4442), (\"be n't\", 4189), ('one of', 3276), ('film be', 3098)]\n"
     ]
    }
   ],
   "source": [
    "def sort(dict, max):\n",
    "    return list(sorted(dict.items(), key=lambda item: -item[1]))[:max]\n",
    "\n",
    "print(sort(ngram_pos_freq[1], 10))\n",
    "print(sort(ngram_neg_freq[1], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_ratio(freq, pos_freq, neg_freq):\n",
    "    pn, nn, gram_ratio = 0, 0, {}\n",
    "    for w in freq:\n",
    "        pn = pos_freq[w] if w in pos_freq else 1\n",
    "        nn = neg_freq[w] if w in neg_freq else 1\n",
    "        ratio = pn/nn\n",
    "        if ratio < 1:\n",
    "            ratio = 1/ratio\n",
    "        gram_ratio[w] = ratio\n",
    "    return gram_ratio\n",
    "\n",
    "\n",
    "def contains_digit(s):\n",
    "    for c in s:\n",
    "        if c.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def process_ngram(freq_dict, pos_dict, neg_dict):\n",
    "    to_remove = set()\n",
    "    for g in freq_dict:\n",
    "        use = True\n",
    "        for t in g.split():\n",
    "            if contains_digit(t):\n",
    "                to_remove.add(g)\n",
    "                break\n",
    "    for g in to_remove:\n",
    "        freq_dict.pop(g)\n",
    "        if g in pos_dict:\n",
    "            pos_dict.pop(g)\n",
    "        if g in neg_dict:\n",
    "            neg_dict.pop(g)\n",
    "        \n",
    "    \n",
    "for i in range(max_gram):\n",
    "    process_ngram(ngram_freq[i], ngram_pos_freq[i], ngram_neg_freq[i])\n",
    "    ngram_ratio[i] = compute_ratio(ngram_freq[i], ngram_pos_freq[i], ngram_neg_freq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('boll', 144.0), ('paulie', 118.0), ('edie', 109.0), ('uwe', 101.0), ('antwone', 84.0), ('thunderbird', 71.0), ('goldsworthy', 65.0), ('beowulf', 60.0), ('gunga', 60.0), ('gypo', 60.0)]\n",
      "[('uwe boll', 88.0), ('rob roy', 86.0), ('i waste', 83.0), ('terrible movie', 79.0), ('this garbage', 74.0), ('this crap', 71.5), ('prom night', 65.0), ('just awful', 61.0), ('be atrocious', 61.0), ('even worth', 60.0)]\n",
      "[('bad film i', 144.0), ('be bad than', 91.0), ('bad movie i', 75.5), ('how bad this', 72.0), ('money on this', 69.0), ('skip this one', 63.0), ('the bad film', 61.49999999999999), ('do not waste', 60.0), ('save your money', 60.0), ('your time on', 59.0)]\n"
     ]
    }
   ],
   "source": [
    "sorted_ngram_ratio = []\n",
    "for i in range(max_gram):\n",
    "    sorted_ngram_ratio.append(sort(ngram_ratio[i], len(ngram_ratio[i])))\n",
    "    print(sorted_ngram_ratio[i][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_to_idx( gram_freq, gram_ratio, low, high, ratio_threshold):\n",
    "    i = 0\n",
    "    gram_idx = {}\n",
    "    for g, f in gram_freq.items():\n",
    "        if f < low or f > high:\n",
    "            continue\n",
    "        r = gram_ratio[g]\n",
    "        if r < ratio_threshold:\n",
    "            continue\n",
    "        gram_idx[g] = i\n",
    "        i += 1\n",
    "    return gram_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(max_gram):\n",
    "    ngram_idx[i] = gram_to_idx(ngram_freq[i], ngram_ratio[i], ngram_low[i], ngram_high[i], ngram_ratio_threshold[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7153\n",
      "13511\n",
      "8978\n"
     ]
    }
   ],
   "source": [
    "d = 0\n",
    "for i in range(max_gram):\n",
    "    print(len(ngram_idx[i]))\n",
    "    d += len(ngram_idx[i]) \n",
    "\n",
    "trainX = np.zeros((pos_n+neg_n, d), dtype=np.float)\n",
    "trainY = np.append(np.ones(pos_n, dtype=np.int), np.zeros(neg_n, dtype=np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_weight(gram_idx, gram_ratio, n):\n",
    "    gram_weight = {}\n",
    "    for g in gram_idx:\n",
    "        r = gram_ratio[g]\n",
    "        gram_weight[g] = r**(0.05*(n+2))\n",
    "    return gram_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(max_gram):\n",
    "    ngram_weight[i] = compute_weight(ngram_idx[i], ngram_ratio[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bad film i', 2.701920077041227), ('be bad than', 2.4649509317268694), ('bad movie i', 2.3745941323740714), ('how bad this', 2.352158045049347), ('money on this', 2.332221626160242), ('skip this one', 2.2901720489235826), ('the bad film', 2.279161096122349), ('do not waste', 2.2679331552660544), ('save your money', 2.2679331552660544), ('your time on', 2.2603224696268156), ('hour of my', 2.2603224696268156), ('my money back', 2.2288073840335185), ('total waste of', 2.2288073840335185), ('not waste your', 2.2123568222761167), ('waste your money', 2.2039445754429603), ('bill and ted', 2.19540189742749), ('minute of my', 2.1867241478865562), ('a total waste', 2.17790642448278), ('the only redeeming', 2.17790642448278), ('excuse for a', 2.173443463961896), ('your time with', 2.159830011764466), ('the bad movie', 2.151494226896409), ('acting be horrible', 2.141127368338324), ('well worth watch', 2.1315255132709487), ('piece of crap', 2.1315255132709487), ('possibly the bad', 2.1315255132709487), ('far the bad', 2.121747460841898), ('also very good', 2.111785764966754), ('so bad i', 2.111785764966754), ('read the script', 2.111785764966754), ('even bad than', 2.1016324782757847), ('bother with this', 2.1016324782757847), ('must see for', 2.096481356314738), ('this pile of', 2.0912791051825463), ('lady from shanghai', 2.0912791051825463), (\"a bug 's\", 2.0912791051825463), ('not even worth', 2.0912791051825463), ('really bad movie', 2.0912791051825463), ('an excellent movie', 2.080716549261844), ('be just awful', 2.080716549261844), ('only good thing', 2.080716549261844), ('be well worth', 2.075353806405558), ('avoid at all', 2.069935054081614), ('watch paint dry', 2.069935054081614), ('your time or', 2.069935054081614), ('excellent a the', 2.058924136478517), ('a wonderful movie', 2.058924136478517), ('acting be awful', 2.058924136478517), ('waste of my', 2.058924136478517), (\"'s not worth\", 2.0476725110792193), ('a terrible film', 2.0476725110792193), ('just plain bad', 2.0476725110792193), ('be awful and', 2.0476725110792193), ('time or money', 2.0476725110792193), ('a terrible movie', 2.0476725110792193), ('be perfectly cast', 2.0361680046403983), ('cast be excellent', 2.0361680046403983), ('for all age', 2.0361680046403983), ('castle in the', 2.0361680046403983), ('waste of film', 2.0361680046403983), ('be very bad', 2.0361680046403983), ('perfect a the', 2.024397458499885), ('the lady from', 2.024397458499885), ('be so awful', 2.024397458499885), ('the young victoria', 2.024397458499885), ('no sense at', 2.024397458499885), ('this movie suck', 2.018408011896243), ('your time watch', 2.0123466170855586), ('on south street', 2.0123466170855586), (\"we 're suppose\", 2.0123466170855586), ('bad i have', 2.0123466170855586), ('complete waste of', 2.006211299752483), ('definitely worth a', 2.0), ('save this movie', 2.0), ('bad film of', 2.0), ('the bad act', 2.0), ('only save grace', 1.9873407546644581), ('avoid this one', 1.9873407546644581), ('a real treat', 1.97435048583482), ('be so poorly', 1.97435048583482), ('total lack of', 1.97435048583482), ('good about this', 1.97435048583482), ('save the movie', 1.97435048583482), ('piece of junk', 1.97435048583482), (\"n't save this\", 1.97435048583482), ('rest of this', 1.97435048583482), ('than watch this', 1.97435048583482), ('and i recommend', 1.961009057454548), ('be brilliant a', 1.961009057454548), ('to save this', 1.961009057454548), ('i fell asleep', 1.961009057454548), ('make absolutely no', 1.961009057454548), ('waste their time', 1.961009057454548), ('dog bite dog', 1.961009057454548), ('be so terrible', 1.961009057454548), ('easily the bad', 1.961009057454548), ('i waste my', 1.961009057454548), ('wait for something', 1.961009057454548), ('movie be awful', 1.961009057454548), ('be a joke', 1.9564801198085815)]\n"
     ]
    }
   ],
   "source": [
    "print(sort(ngram_weight[2], 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(ngram, i, ngram_idx, ngram_weight):\n",
    "    encoded = np.zeros(d, dtype=np.float)\n",
    "    st = 0\n",
    "    for n in range(max_gram):\n",
    "        gs = ngram[n][i]\n",
    "        idx_dict = ngram_idx[n]\n",
    "        weight_dict = ngram_weight[n]\n",
    "        decay = 0.5\n",
    "        discount = {}\n",
    "        for g in gs:\n",
    "            if g not in idx_dict:\n",
    "                continue\n",
    "            disc = 1\n",
    "            if g in discount:\n",
    "                disc = discount[g]\n",
    "            else:\n",
    "                discount[g] = 1\n",
    "            encoded[idx_dict[g]+st] += disc*weight_dict[g]\n",
    "            discount[g] *= decay\n",
    "        st += len(idx_dict)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_encode(trainX):\n",
    "    for i in range(pos_n):\n",
    "        trainX[i] = encode(ngram_pos, i, ngram_idx, ngram_weight)\n",
    "    for i in range(neg_n):\n",
    "        trainX[i + pos_n] = encode(ngram_neg, i, ngram_idx, ngram_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert trainset to encoded vectors\n",
    "batch_encode(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the trainset\n",
    "indices = np.arange(pos_n+neg_n)\n",
    "np.random.shuffle(indices)\n",
    "trainX = trainX[indices]\n",
    "trainY = trainY[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  1.07516943,  1.09149343,  1.09199281,  1.09202729,\n",
       "        1.09337374,  1.09450312,  1.09724081,  1.09808844,  1.10553592,\n",
       "        1.11612317,  1.12349136,  1.12981083,  1.13087857,  1.14282139,\n",
       "        1.14869835,  1.16275263,  1.18022078,  1.19680497,  1.20097687,\n",
       "        1.20673027,  1.21087631,  1.24103837,  1.25308888,  1.26724697,\n",
       "        1.27305012,  1.29138103,  1.29400731,  1.32131546,  1.33784144,\n",
       "        1.33895107,  1.35096004,  1.36082211,  1.36604026,  1.37416605,\n",
       "        1.39038917,  1.43096908,  1.45225261,  1.51571657,  1.55184557,\n",
       "        1.6140618 ,  1.62053674,  1.62222457,  1.86358668,  1.87560356])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(C=1000, solver='sag', max_iter=100)\n",
    "\n",
    "lr_clf.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junzhiwa/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf_2 = LogisticRegression(C=1000, solver='sag', max_iter=100)\n",
    "\n",
    "lr_clf_2.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testX = np.zeros((test_n, d), dtype=np.float)\n",
    "ngram_test = [[] for i in range(max_gram)]\n",
    "\n",
    "\n",
    "def process_test_files(files, dir):\n",
    "    for file in files:\n",
    "        with open(os.path.join(dir, file), 'r') as f:\n",
    "            review = f.readline()\n",
    "            res = process_sent(review, max_gram)\n",
    "            for i in range(max_gram):\n",
    "                ngram_test[i].append(res[i])\n",
    "                \n",
    "\n",
    "process_test_files(test_fs, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99924000000000002"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97855999999999999"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf_2.score(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_encode_test(testX):\n",
    "    for i in range(len(testX)):\n",
    "        testX[i] = encode(ngram_test, i, ngram_idx, ngram_weight)\n",
    "\n",
    "batch_encode_test(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  1.07683846,  1.08360799,  1.08398133,  1.09622524,\n",
       "        1.10527855,  1.1320792 ,  1.14021705,  1.16275263,  1.21271526,\n",
       "        1.22529527,  1.23114441,  1.24186849,  1.24504869,  1.26527867,\n",
       "        1.28056168,  1.34832695,  1.41884047,  1.43096908,  1.43287697,\n",
       "        1.46061751,  1.46186644,  1.47577316,  1.48956783,  1.50536863,\n",
       "        1.57312444,  1.70911529,  1.83841629,  1.96600624,  1.98734075,\n",
       "        2.18941675])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(testX[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = lr_clf_2.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([5385, 5615]))"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {'id':np.arange(11000), 'labels':pred }\n",
    "df = pd.DataFrame(data=data)\n",
    "df.to_csv('res_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
